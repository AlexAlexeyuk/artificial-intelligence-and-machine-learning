{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Optimization Algorithms\n",
    "In this notebook you can find a collection of GD based optimization algorithm used for deep learning. The code is always accompanied by a explanatory youtube video which are linked here:\n",
    "- [Stochastic Gradient Descent](https://youtu.be/IH9kqpMORLM)\n",
    "- [Stochastic Gradient Descent + Momentum](https://youtu.be/7EuiXb6hFAM)\n",
    "- [Adagrad](https://youtu.be/EGt-UOIIdDk)\n",
    "\n",
    "## Tests\n",
    "In order to demonstrate the algorithms capabilities to optimize a function we used these simple test setup:\n",
    "- learning various linear function of the form `f(x) = w0 + w1*x` with the squared error. This is a simple sanity check as the gradient are simple to calculate and the test data is also easy to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import permutation\n",
    "\n",
    "class Line():\n",
    "    def __init__(self):\n",
    "        self.w0 = np.random.uniform(0,1,1)\n",
    "        self.w1 = np.random.uniform(0,1,1)\n",
    "    \n",
    "    def evaluate(self,x):\n",
    "        \"\"\"\n",
    "            evaluate: will evaluate the line yhate given x\n",
    "            x: a point in the plane\n",
    "\n",
    "            return the result of the function evalutation\n",
    "        \"\"\"\n",
    "        return self.w0 + self.w1*x\n",
    "    \n",
    "    def dx_w0(self, x, y):\n",
    "        \"\"\"\n",
    "            dx_w0: partial derivative of the weight w0\n",
    "            x: a point in the plane\n",
    "            y: the response of the point x\n",
    "\n",
    "            return the gradient at that point for this x and y for w0\n",
    "        \"\"\"\n",
    "        yhat = self.evaluate(x)\n",
    "        return 2*(yhat - y)\n",
    "        \n",
    "    \n",
    "    def dx_w1(self, x, y):\n",
    "        \"\"\"\n",
    "            dx_w1: partial derivative of the weight w1 for a linear function\n",
    "            x: a point in the plane\n",
    "            y: the response of the point x\n",
    "\n",
    "            return the gradient at that point for this x and y for w1\n",
    "        \"\"\"  \n",
    "        yhat = self.evaluate(x)\n",
    "        return 2*x*(yhat - y)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"y = {self.w0[0]} + {self.w1[0]}*x\"\n",
    "    \n",
    "    \n",
    "#################### Helper functions ######################\n",
    "def stochastic_sample(xs, ys):\n",
    "    \"\"\"\n",
    "        stochastic_sample: sample with replacement one x and one y\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        \n",
    "        return the randomly selected x and y point\n",
    "    \"\"\"\n",
    "    perm = permutation(len(xs))\n",
    "    x = xs[perm[0]]\n",
    "    y = ys[perm[0]]\n",
    "\n",
    "    return x, y\n",
    "    \n",
    "    \n",
    "def gradient(dx, xs, ys):\n",
    "    \"\"\"\n",
    "        gradient: estimate mean gradient over all point for w1\n",
    "        dx: partial derivative function used to evaluate the gradient\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        \n",
    "        return the mean gradient all x and y for w1\n",
    "    \"\"\"         \n",
    "    N = len(ys)\n",
    "    \n",
    "    total = 0\n",
    "    for x,y in zip(xs,ys):\n",
    "        total = total + dx(x, y)\n",
    "    \n",
    "    gradient = total/N\n",
    "    return gradient\n",
    "\n",
    "################## Optimization Functions #####################\n",
    "\n",
    "def gd(model, xs, ys, learning_rate = 0.01, max_num_iteration = 1000):\n",
    "    \"\"\"\n",
    "        gd: will estimate the parameters w1 and w2 (here it uses least square cost function)\n",
    "        model: the model we are trying to optimize using gradient descent\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        learning_rate: the learning rate for the step that weights update will take\n",
    "        max_num_iteration: the number of iteration before we stop updating\n",
    "    \"\"\"    \n",
    "\n",
    "    for i in range(max_num_iteration):\n",
    "        model.w0 = model.w0 - learning_rate*gradient(model.dx_w0, xs, ys)\n",
    "        model.w1 = model.w1 - learning_rate*gradient(model.dx_w1, xs, ys)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}\")\n",
    "            print(model)\n",
    "            \n",
    "def sgd(model, xs, ys, learning_rate = 0.01, max_num_iteration = 1000):\n",
    "    \"\"\"\n",
    "        sgd: will estimate the parameters w0 and w1 \n",
    "        (here it uses least square cost function)\n",
    "        model: the model we are trying to optimize using sgd\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        learning_rate: the learning rate for the step that weights update will take\n",
    "        max_num_iteration: the number of iteration before we stop updating\n",
    "    \"\"\"       \n",
    "    \n",
    "    for i in range(max_num_iteration):\n",
    "        \n",
    "        # Select a random x and y\n",
    "        x, y = stochastic_sample(xs, ys)\n",
    "        \n",
    "        # Updating the model parameters\n",
    "        model.w0 = model.w0 - learning_rate*model.dx_w0(x, y)\n",
    "        model.w1 = model.w1 - learning_rate*model.dx_w1(x, y)\n",
    "        \n",
    "    \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}\")\n",
    "            print(model)\n",
    "            \n",
    "def sgd_momentum(model, xs, ys, learning_rate = 0.01, decay_factor = 0.9, max_num_iteration = 1000):\n",
    "    \"\"\"\n",
    "        sgd_momentum: will estimate the parameters w0 and w1 \n",
    "        (here it uses least square cost function)\n",
    "        model: the model we are trying to optimize using sgd\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        learning_rate: the learning rate for the step that weights update will take\n",
    "        decay_factor: determines the relative contribution of the current gradient and earlier gradients to the weight change\n",
    "        max_num_iteration: the number of iteration before we stop updating\n",
    "    \"\"\"\n",
    "    \n",
    "    # These are needed to keep track of the previous gradient\n",
    "    prev_g0 = 0\n",
    "    prev_g1 = 0\n",
    "    \n",
    "    for i in range(max_num_iteration):\n",
    "        \n",
    "        # Select a random x and y\n",
    "        x, y = stochastic_sample(xs, ys)\n",
    "\n",
    "        g0 = decay_factor*prev_g0 - learning_rate*model.dx_w0(x,y)\n",
    "        g1 = decay_factor*prev_g1 - learning_rate*model.dx_w1(x,y)\n",
    "        \n",
    "        # Updating the model parameters\n",
    "        model.w0 = model.w0 + g0\n",
    "        model.w1 = model.w1 + g1\n",
    "        \n",
    "        # swap previous gradient\n",
    "        prev_g0, prev_g1 = g0, g1\n",
    "    \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}\")\n",
    "            print(model)\n",
    "            \n",
    "            \n",
    "def adagrad(model, xs, ys, learning_rate = 0.1, max_num_iteration = 10000, eps=0.0000001):\n",
    "    \"\"\"\n",
    "        adagrad: will estimate the parameters w0 and w1 \n",
    "        (here it uses least square cost function)\n",
    "        model: the model we are trying to optimize using sgd\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        learning_rate: the learning rate for the step that weights update will take\n",
    "        max_num_iteration: the number of iteration before we stop updating\n",
    "        eps: is a numerical safety to avoid division by 0\n",
    "    \"\"\"         \n",
    "    # Here only the diagonal matter\n",
    "    G = [[0,0],\n",
    "         [0,0]]\n",
    "    \n",
    "    for i in range(max_num_iteration):\n",
    "        \n",
    "        # Select a random x and y\n",
    "        x, y = stochastic_sample(xs, ys)\n",
    "        \n",
    "        g0 = model.dx_w0(x, y)\n",
    "        g1 = model.dx_w1(x, y)\n",
    "        \n",
    "        G[0][0] = G[0][0] + g0*g0\n",
    "        G[1][1] = G[1][1] + g1*g1\n",
    "        \n",
    "        model.w0 = model.w0 - (learning_rate/np.sqrt(G[0][0] + eps)) * g0\n",
    "        model.w1 = model.w1 - (learning_rate/np.sqrt(G[1][1] + eps)) * g1\n",
    "    \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}\")\n",
    "            print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent: \n",
      "Iteration 0\n",
      "y = 0.1484230627743015 + 0.6232288191403248*x\n",
      "Iteration 100\n",
      "y = 0.14660574025182332 + 0.9705053945109118*x\n",
      "Iteration 200\n",
      "y = 0.0991315358376387 + 0.9800564047762639*x\n",
      "Iteration 300\n",
      "y = 0.06703053632585736 + 0.9865145851638741*x\n",
      "Iteration 400\n",
      "y = 0.04532455552278566 + 0.9908814628825817*x\n",
      "Iteration 500\n",
      "y = 0.03064745481598692 + 0.9938342483214537*x\n",
      "Iteration 600\n",
      "y = 0.020723126258254662 + 0.9958308560603568*x\n",
      "Iteration 700\n",
      "y = 0.01401251635720002 + 0.9971809177379064*x\n",
      "Iteration 800\n",
      "y = 0.009474951424502668 + 0.9980937993709253*x\n",
      "Iteration 900\n",
      "y = 0.006406751093678868 + 0.9987110695962501*x\n",
      "y = 0.004349087072231295 + 0.9991250369377569*x\n",
      "Stochastic Gradient Descent: \n",
      "Iteration 0\n",
      "y = 0.5688755138166692 + 0.5107534412165847*x\n",
      "Iteration 100\n",
      "y = 0.4425741941184875 + 0.9361376701582683*x\n",
      "Iteration 200\n",
      "y = 0.314871672875429 + 0.9469777307465838*x\n",
      "Iteration 300\n",
      "y = 0.21571168770394028 + 0.9649569866226333*x\n",
      "Iteration 400\n",
      "y = 0.15004050569910096 + 0.9696567148974441*x\n",
      "Iteration 500\n",
      "y = 0.0988129784307545 + 0.9775234957727947*x\n",
      "Iteration 600\n",
      "y = 0.07007969477221268 + 0.9806522734189129*x\n",
      "Iteration 700\n",
      "y = 0.04966042765505737 + 0.991419421733979*x\n",
      "Iteration 800\n",
      "y = 0.03231364440935814 + 0.993310221241273*x\n",
      "Iteration 900\n",
      "y = 0.02276769521896498 + 0.9941396140762522*x\n",
      "y = 0.015405747170678294 + 0.9969804888018872*x\n",
      "SGD + Momentum: \n",
      "Iteration 0\n",
      "y = 0.21611831953891156 + 0.3527465284644882*x\n",
      "Iteration 100\n",
      "y = 0.35969464275694724 + 0.8023013207849262*x\n",
      "Iteration 200\n",
      "y = 0.7182476979987475 + 0.5632919996162326*x\n",
      "Iteration 300\n",
      "y = 0.08999980965909801 + 1.0799148111049233*x\n",
      "Iteration 400\n",
      "y = -0.016984609565546475 + 0.9921229181688871*x\n",
      "Iteration 500\n",
      "y = -0.023404847337009413 + 0.9927386134564401*x\n",
      "Iteration 600\n",
      "y = 0.02041923841633324 + 0.9996920669070389*x\n",
      "Iteration 700\n",
      "y = 0.16841491586927287 + 0.846678915946645*x\n",
      "Iteration 800\n",
      "y = 0.5477353158437553 + 0.6298429760841683*x\n",
      "Iteration 900\n",
      "y = 0.08230618289579508 + 1.0093356650058198*x\n",
      "y = 0.023696519260703566 + 0.9891299933076861*x\n",
      "Adagrad\n",
      "Iteration 0\n",
      "y = 1.0574579842057295 + 0.6317941000274044*x\n",
      "Iteration 100\n",
      "y = 0.7056031179671088 + 0.8710186004033444*x\n",
      "Iteration 200\n",
      "y = 0.4978329030850646 + 0.9032534696162404*x\n",
      "Iteration 300\n",
      "y = 0.3703875197209583 + 0.9178233529216375*x\n",
      "Iteration 400\n",
      "y = 0.26589089516826864 + 0.9454579787390859*x\n",
      "Iteration 500\n",
      "y = 0.20246241106205795 + 0.958296158983464*x\n",
      "Iteration 600\n",
      "y = 0.15169567256567595 + 0.9681815250967152*x\n",
      "Iteration 700\n",
      "y = 0.1119661359625804 + 0.9792048232707626*x\n",
      "Iteration 800\n",
      "y = 0.08250924412592951 + 0.9845432378100701*x\n",
      "Iteration 900\n",
      "y = 0.0583055006342606 + 0.9858544822699331*x\n",
      "Iteration 1000\n",
      "y = 0.045553223395587136 + 0.9913794935451248*x\n",
      "Iteration 1100\n",
      "y = 0.03224921302430989 + 0.9941922798345326*x\n",
      "Iteration 1200\n",
      "y = 0.022369954186756996 + 0.9948521080972246*x\n",
      "Iteration 1300\n",
      "y = 0.017274645132686846 + 0.9961615882526326*x\n",
      "Iteration 1400\n",
      "y = 0.013103081044554212 + 0.997199164700328*x\n",
      "Iteration 1500\n",
      "y = 0.00996084575543066 + 0.9979302390877486*x\n",
      "Iteration 1600\n",
      "y = 0.0073570341110758344 + 0.9984116048803435*x\n",
      "Iteration 1700\n",
      "y = 0.005655818186335868 + 0.9990096822030707*x\n",
      "Iteration 1800\n",
      "y = 0.004313099725999993 + 0.9990905641764424*x\n",
      "Iteration 1900\n",
      "y = 0.003418226779322752 + 0.9993655714662771*x\n",
      "Iteration 2000\n",
      "y = 0.0024827988041676604 + 0.9993985611455867*x\n",
      "Iteration 2100\n",
      "y = 0.0017874708940749029 + 0.9996306928484112*x\n",
      "Iteration 2200\n",
      "y = 0.0014263301371659086 + 0.9997322995035691*x\n",
      "Iteration 2300\n",
      "y = 0.0010480886781319143 + 0.9997767702805507*x\n",
      "Iteration 2400\n",
      "y = 0.000782152603651157 + 0.9998435664146448*x\n",
      "Iteration 2500\n",
      "y = 0.0005587715346629678 + 0.999877896953173*x\n",
      "Iteration 2600\n",
      "y = 0.0004204073253513165 + 0.9999163020578808*x\n",
      "Iteration 2700\n",
      "y = 0.0003096846621313471 + 0.9999325338081024*x\n",
      "Iteration 2800\n",
      "y = 0.00022254432581524425 + 0.9999406400811458*x\n",
      "Iteration 2900\n",
      "y = 0.0001709058339407168 + 0.9999664635813308*x\n",
      "Iteration 3000\n",
      "y = 0.00012645033162310537 + 0.9999756951694397*x\n",
      "Iteration 3100\n",
      "y = 9.307622582644783e-05 + 0.9999805606680482*x\n",
      "Iteration 3200\n",
      "y = 7.022776577608908e-05 + 0.9999840447066879*x\n",
      "Iteration 3300\n",
      "y = 5.3498946065673307e-05 + 0.9999896111638994*x\n",
      "Iteration 3400\n",
      "y = 4.1638894820071105e-05 + 0.9999906717905516*x\n",
      "Iteration 3500\n",
      "y = 3.0405179779504795e-05 + 0.9999932254970882*x\n",
      "Iteration 3600\n",
      "y = 2.3806725667356553e-05 + 0.9999956124530497*x\n",
      "Iteration 3700\n",
      "y = 1.781890849702071e-05 + 0.9999960123806192*x\n",
      "Iteration 3800\n",
      "y = 1.3497802210535862e-05 + 0.9999973448712727*x\n",
      "Iteration 3900\n",
      "y = 9.537757438461499e-06 + 0.9999976674868205*x\n",
      "Iteration 4000\n",
      "y = 7.19813769532751e-06 + 0.9999985640110396*x\n",
      "Iteration 4100\n",
      "y = 5.30063277546252e-06 + 0.9999988860182822*x\n",
      "Iteration 4200\n",
      "y = 3.812223594714383e-06 + 0.9999992236026126*x\n",
      "Iteration 4300\n",
      "y = 2.967302863351052e-06 + 0.9999993471843662*x\n",
      "Iteration 4400\n",
      "y = 2.1857948578123124e-06 + 0.9999995536269083*x\n",
      "Iteration 4500\n",
      "y = 1.611971132337144e-06 + 0.999999664335356*x\n",
      "Iteration 4600\n",
      "y = 1.2114601746907248e-06 + 0.9999997610635348*x\n",
      "Iteration 4700\n",
      "y = 9.206927667729321e-07 + 0.9999998108035976*x\n",
      "Iteration 4800\n",
      "y = 6.567445411255911e-07 + 0.9999998599347418*x\n",
      "Iteration 4900\n",
      "y = 4.753367998316269e-07 + 0.9999998945842601*x\n",
      "Iteration 5000\n",
      "y = 3.2346391583184345e-07 + 0.9999999244422623*x\n",
      "Iteration 5100\n",
      "y = 2.565345588303061e-07 + 0.9999999519864401*x\n",
      "Iteration 5200\n",
      "y = 1.9225220567260504e-07 + 0.9999999602376045*x\n",
      "Iteration 5300\n",
      "y = 1.341474501386698e-07 + 0.9999999729297641*x\n",
      "Iteration 5400\n",
      "y = 1.0225747218020893e-07 + 0.9999999805424208*x\n",
      "Iteration 5500\n",
      "y = 7.751946039250485e-08 + 0.9999999849128712*x\n",
      "Iteration 5600\n",
      "y = 5.653491931189889e-08 + 0.9999999886942555*x\n",
      "Iteration 5700\n",
      "y = 3.9623556873731375e-08 + 0.9999999913315879*x\n",
      "Iteration 5800\n",
      "y = 2.822153556573533e-08 + 0.9999999941293756*x\n",
      "Iteration 5900\n",
      "y = 2.0789563709078894e-08 + 0.9999999957254149*x\n",
      "Iteration 6000\n",
      "y = 1.5456536172570448e-08 + 0.9999999969031944*x\n",
      "Iteration 6100\n",
      "y = 1.1233123864575246e-08 + 0.9999999974435688*x\n",
      "Iteration 6200\n",
      "y = 8.26008281007707e-09 + 0.9999999981377299*x\n",
      "Iteration 6300\n",
      "y = 5.922353317697743e-09 + 0.9999999986384049*x\n",
      "Iteration 6400\n",
      "y = 4.356385066922961e-09 + 0.9999999991587588*x\n",
      "Iteration 6500\n",
      "y = 3.152709325175789e-09 + 0.9999999993083587*x\n",
      "Iteration 6600\n",
      "y = 2.1457625093844855e-09 + 0.9999999995309099*x\n",
      "Iteration 6700\n",
      "y = 1.6701733749350575e-09 + 0.9999999996525907*x\n",
      "Iteration 6800\n",
      "y = 1.2631943624048844e-09 + 0.9999999997613972*x\n",
      "Iteration 6900\n",
      "y = 9.522533072959325e-10 + 0.9999999998236411*x\n",
      "Iteration 7000\n",
      "y = 7.025990330353733e-10 + 0.9999999998533469*x\n",
      "Iteration 7100\n",
      "y = 4.952666829476252e-10 + 0.99999999988945*x\n",
      "Iteration 7200\n",
      "y = 3.8145795760454e-10 + 0.9999999999238872*x\n",
      "Iteration 7300\n",
      "y = 2.6392772028115095e-10 + 0.9999999999410957*x\n",
      "Iteration 7400\n",
      "y = 1.9705065317286797e-10 + 0.9999999999573081*x\n",
      "Iteration 7500\n",
      "y = 1.4911386380065397e-10 + 0.9999999999667711*x\n",
      "Iteration 7600\n",
      "y = 1.1240483581686058e-10 + 0.9999999999766949*x\n",
      "Iteration 7700\n",
      "y = 7.83490979896162e-11 + 0.9999999999800465*x\n",
      "Iteration 7800\n",
      "y = 5.998568788901562e-11 + 0.9999999999865015*x\n",
      "Iteration 7900\n",
      "y = 4.324089416505722e-11 + 0.9999999999908508*x\n",
      "Iteration 8000\n",
      "y = 3.184385777744211e-11 + 0.9999999999937257*x\n",
      "Iteration 8100\n",
      "y = 2.3585944270181393e-11 + 0.9999999999952467*x\n",
      "Iteration 8200\n",
      "y = 1.6682676217001865e-11 + 0.9999999999959973*x\n",
      "Iteration 8300\n",
      "y = 1.3258398707309796e-11 + 0.9999999999976545*x\n",
      "Iteration 8400\n",
      "y = 9.32239186955709e-12 + 0.9999999999983057*x\n",
      "Iteration 8500\n",
      "y = 6.9093546701797965e-12 + 0.9999999999984646*x\n",
      "Iteration 8600\n",
      "y = 5.364538749289996e-12 + 0.9999999999989297*x\n",
      "Iteration 8700\n",
      "y = 3.861790772693636e-12 + 0.9999999999991814*x\n",
      "Iteration 8800\n",
      "y = 2.813105574786122e-12 + 0.9999999999993425*x\n",
      "Iteration 8900\n",
      "y = 2.2143242762197007e-12 + 0.9999999999995742*x\n",
      "Iteration 9000\n",
      "y = 1.5902677721123191e-12 + 0.9999999999997194*x\n",
      "Iteration 9100\n",
      "y = 1.1219240530889449e-12 + 0.9999999999997637*x\n",
      "Iteration 9200\n",
      "y = 8.990328262635242e-13 + 0.9999999999998265*x\n",
      "Iteration 9300\n",
      "y = 6.371095247675532e-13 + 0.9999999999998653*x\n",
      "Iteration 9400\n",
      "y = 5.091425666192067e-13 + 0.9999999999999023*x\n",
      "Iteration 9500\n",
      "y = 3.7913921750310554e-13 + 0.9999999999999221*x\n",
      "Iteration 9600\n",
      "y = 2.8490372812394907e-13 + 0.9999999999999485*x\n",
      "Iteration 9700\n",
      "y = 2.04687143476855e-13 + 0.9999999999999563*x\n",
      "Iteration 9800\n",
      "y = 1.4877997600733474e-13 + 0.9999999999999679*x\n",
      "Iteration 9900\n",
      "y = 1.1321051465511633e-13 + 0.9999999999999775*x\n",
      "y = 8.215180903110497e-14 + 0.9999999999999855*x\n"
     ]
    }
   ],
   "source": [
    "# Here we have a simple line with intercept = 0 and slope = 1\n",
    "xs = [1,2,3,4,5,6,7]\n",
    "ys = [1,2,3,4,5,6,7]\n",
    "\n",
    "# Gradient Descent\n",
    "model = Line()\n",
    "print(\"Gradient Descent: \")\n",
    "gd(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "model = Line()\n",
    "print(\"Stochastic Gradient Descent: \")\n",
    "sgd(model, xs, ys)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have a simple line with intercept = 0 and slope = 2\n",
    "xs = [1,2,3,4,5,6,7]\n",
    "ys = [2,4,6,8,10,12,14]\n",
    "\n",
    "# Gradient Descent\n",
    "model = Line()\n",
    "print(\"Gradient Descent: \")\n",
    "gradient_descent(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "model = Line()\n",
    "print(\"Stochastic Gradient Descent: \")\n",
    "stochastic_gradient_descent(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Stochastic Gradient Descent with Momentum\n",
    "model = Line()\n",
    "print(\"SGD + Momentum: \")\n",
    "sgd_momentum(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Adagrad\n",
    "model = Line()\n",
    "print(\"Adagrad\")\n",
    "adagrad(model, xs, ys)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent: \n",
      "Iteration 0\n",
      "y = 0.665189371668105 + 0.9077676569256095*x\n",
      "Iteration 100\n",
      "y = 0.9281890927973836 + 2.0144471449352293*x\n",
      "Iteration 200\n",
      "y = 0.9514430641756475 + 2.009768837587384*x\n",
      "Iteration 300\n",
      "y = 0.96716688162708 + 2.006605470370549*x\n",
      "Iteration 400\n",
      "y = 0.9777989767313627 + 2.004466471924208*x\n",
      "Iteration 500\n",
      "y = 0.9849881626053194 + 2.0030201288221177*x\n",
      "Iteration 600\n",
      "y = 0.9898493299503595 + 2.002042143834544*x\n",
      "Iteration 700\n",
      "y = 0.9931363430239945 + 2.001380852171081*x\n",
      "Iteration 800\n",
      "y = 0.9953589480444256 + 2.0009337014788704*x\n",
      "Iteration 900\n",
      "y = 0.9968618240495349 + 2.0006313481413165*x\n",
      "y = 0.9978697158267908 + 2.000428577292816*x\n",
      "Stochastic Gradient Descent: \n",
      "Iteration 0\n",
      "y = 0.3411892027614143 + 1.770977165160403*x\n",
      "Iteration 100\n",
      "y = 0.6039940977243495 + 2.102681740225482*x\n",
      "Iteration 200\n",
      "y = 0.7205112221946766 + 2.0588950933611394*x\n",
      "Iteration 300\n",
      "y = 0.8011732178881769 + 2.0392396655904825*x\n",
      "Iteration 400\n",
      "y = 0.8719426224199744 + 2.024874311328534*x\n",
      "Iteration 500\n",
      "y = 0.9068175516948642 + 2.0167703182721874*x\n",
      "Iteration 600\n",
      "y = 0.9379964368043875 + 2.0105436579733205*x\n",
      "Iteration 700\n",
      "y = 0.958175128126369 + 2.006082535745101*x\n",
      "Iteration 800\n",
      "y = 0.9724672989708719 + 2.003933565338797*x\n",
      "Iteration 900\n",
      "y = 0.9810183303887587 + 2.004096327154227*x\n",
      "y = 0.986845613291048 + 2.0018935308024353*x\n"
     ]
    }
   ],
   "source": [
    "# Here we have a simple line with intercept = 1 and slope = 2\n",
    "xs = [1,2,3,4,5,6,7]\n",
    "ys = [3,5,7,9,11,13,15]\n",
    "\n",
    "# Gradient Descent\n",
    "model = Line()\n",
    "print(\"Gradient Descent: \")\n",
    "gradient_descent(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "model = Line()\n",
    "print(\"Stochastic Gradient Descent: \")\n",
    "stochastic_gradient_descent(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Stochastic Gradient Descent with Momentum\n",
    "model = Line()\n",
    "print(\"SGD + Momentum: \")\n",
    "sgd_momentum(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Adagrad\n",
    "model = Line()\n",
    "print(\"Adagrad\")\n",
    "adagrad(model, xs, ys)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
