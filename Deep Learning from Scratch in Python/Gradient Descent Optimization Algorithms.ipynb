{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Optimization Algorithms\n",
    "In this notebook you can find a collection of GD based optimization algorithm used for deep learning. The code is always accompanied by a explanatory youtube video which are linked here:\n",
    "- [Stochastic Gradient Descent](https://youtu.be/IH9kqpMORLM)\n",
    "- [Stochastic Gradient Descent + Momentum](https://youtu.be/7EuiXb6hFAM)\n",
    "- [Adagrad](https://youtu.be/EGt-UOIIdDk)\n",
    "- [RMSprop](https://youtu.be/nLCuzsQaAKE)\n",
    "- [AdaDelta](https://youtu.be/6gvh0IySNMs)\n",
    "- Adam\n",
    "\n",
    "## Tests\n",
    "In order to demonstrate the algorithms capabilities to optimize a function we used these simple test setup:\n",
    "- learning various linear function of the form `f(x) = w0 + w1*x` with the squared error. This is a simple sanity check as the gradient are simple to calculate and the test data is also easy to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import permutation\n",
    "\n",
    "class Line():\n",
    "    def __init__(self):\n",
    "        self.w0 = np.random.uniform(0,1,1)\n",
    "        self.w1 = np.random.uniform(0,1,1)\n",
    "    \n",
    "    def evaluate(self,x):\n",
    "        \"\"\"\n",
    "            evaluate: will evaluate the line yhate given x\n",
    "            x: a point in the plane\n",
    "\n",
    "            return the result of the function evalutation\n",
    "        \"\"\"\n",
    "        return self.w0 + self.w1*x\n",
    "    \n",
    "    def dx_w0(self, x, y):\n",
    "        \"\"\"\n",
    "            dx_w0: partial derivative of the weight w0\n",
    "            x: a point in the plane\n",
    "            y: the response of the point x\n",
    "\n",
    "            return the gradient at that point for this x and y for w0\n",
    "        \"\"\"\n",
    "        yhat = self.evaluate(x)\n",
    "        return 2*(yhat - y)\n",
    "        \n",
    "    \n",
    "    def dx_w1(self, x, y):\n",
    "        \"\"\"\n",
    "            dx_w1: partial derivative of the weight w1 for a linear function\n",
    "            x: a point in the plane\n",
    "            y: the response of the point x\n",
    "\n",
    "            return the gradient at that point for this x and y for w1\n",
    "        \"\"\"  \n",
    "        yhat = self.evaluate(x)\n",
    "        return 2*x*(yhat - y)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"y = {self.w0[0]} + {self.w1[0]}*x\"\n",
    "    \n",
    "    \n",
    "#################### Helper functions ######################\n",
    "def stochastic_sample(xs, ys):\n",
    "    \"\"\"\n",
    "        stochastic_sample: sample with replacement one x and one y\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        \n",
    "        return the randomly selected x and y point\n",
    "    \"\"\"\n",
    "    perm = permutation(len(xs))\n",
    "    x = xs[perm[0]]\n",
    "    y = ys[perm[0]]\n",
    "\n",
    "    return x, y\n",
    "    \n",
    "    \n",
    "def gradient(dx, xs, ys):\n",
    "    \"\"\"\n",
    "        gradient: estimate mean gradient over all point for w1\n",
    "        dx: partial derivative function used to evaluate the gradient\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        \n",
    "        return the mean gradient all x and y for w1\n",
    "    \"\"\"         \n",
    "    N = len(ys)\n",
    "    \n",
    "    total = 0\n",
    "    for x,y in zip(xs,ys):\n",
    "        total = total + dx(x, y)\n",
    "    \n",
    "    gradient = total/N\n",
    "    return gradient\n",
    "\n",
    "################## Optimization Functions #####################\n",
    "\n",
    "def gd(model, xs, ys, learning_rate = 0.01, max_num_iteration = 1000):\n",
    "    \"\"\"\n",
    "        gd: will estimate the parameters w1 and w2 (here it uses least square cost function)\n",
    "        model: the model we are trying to optimize using gradient descent\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        learning_rate: the learning rate for the step that weights update will take\n",
    "        max_num_iteration: the number of iteration before we stop updating\n",
    "    \"\"\"    \n",
    "\n",
    "    for i in range(max_num_iteration):\n",
    "        model.w0 = model.w0 - learning_rate*gradient(model.dx_w0, xs, ys)\n",
    "        model.w1 = model.w1 - learning_rate*gradient(model.dx_w1, xs, ys)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}\")\n",
    "            print(model)\n",
    "            \n",
    "def sgd(model, xs, ys, learning_rate = 0.01, max_num_iteration = 1000):\n",
    "    \"\"\"\n",
    "        sgd: will estimate the parameters w0 and w1 \n",
    "        (here it uses least square cost function)\n",
    "        model: the model we are trying to optimize using sgd\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        learning_rate: the learning rate for the step that weights update will take\n",
    "        max_num_iteration: the number of iteration before we stop updating\n",
    "    \"\"\"       \n",
    "    \n",
    "    for i in range(max_num_iteration):\n",
    "        \n",
    "        # Select a random x and y\n",
    "        x, y = stochastic_sample(xs, ys)\n",
    "        \n",
    "        # Updating the model parameters\n",
    "        model.w0 = model.w0 - learning_rate*model.dx_w0(x, y)\n",
    "        model.w1 = model.w1 - learning_rate*model.dx_w1(x, y)\n",
    "        \n",
    "    \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}\")\n",
    "            print(model)\n",
    "            \n",
    "def sgd_momentum(model, xs, ys, learning_rate = 0.01, decay_factor = 0.9, max_num_iteration = 1000):\n",
    "    \"\"\"\n",
    "        sgd_momentum: will estimate the parameters w0 and w1 \n",
    "        (here it uses least square cost function)\n",
    "        model: the model we are trying to optimize using sgd\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        learning_rate: the learning rate for the step that weights update will take\n",
    "        decay_factor: determines the relative contribution of the current gradient and earlier gradients to the weight change\n",
    "        max_num_iteration: the number of iteration before we stop updating\n",
    "    \"\"\"\n",
    "    \n",
    "    # These are needed to keep track of the previous gradient\n",
    "    prev_g0 = 0\n",
    "    prev_g1 = 0\n",
    "    \n",
    "    for i in range(max_num_iteration):\n",
    "        \n",
    "        # Select a random x and y\n",
    "        x, y = stochastic_sample(xs, ys)\n",
    "\n",
    "        g0 = decay_factor*prev_g0 - learning_rate*model.dx_w0(x,y)\n",
    "        g1 = decay_factor*prev_g1 - learning_rate*model.dx_w1(x,y)\n",
    "        \n",
    "        # Updating the model parameters\n",
    "        model.w0 = model.w0 + g0\n",
    "        model.w1 = model.w1 + g1\n",
    "        \n",
    "        # swap previous gradient\n",
    "        prev_g0, prev_g1 = g0, g1\n",
    "    \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}\")\n",
    "            print(model)\n",
    "            \n",
    "            \n",
    "def adagrad(model, xs, ys, learning_rate = 0.1, max_num_iteration = 10000, eps=0.0000001):\n",
    "    \"\"\"\n",
    "        adagrad: will estimate the parameters w0 and w1 \n",
    "        (here it uses least square cost function)\n",
    "        model: the model we are trying to optimize using sgd\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        learning_rate: the learning rate for the step that weights update will take\n",
    "        max_num_iteration: the number of iteration before we stop updating\n",
    "        eps: is a numerical safety to avoid division by 0\n",
    "    \"\"\"         \n",
    "    # Here only the diagonal matter\n",
    "    G = [[0,0],\n",
    "         [0,0]]\n",
    "    \n",
    "    for i in range(max_num_iteration):\n",
    "        \n",
    "        # Select a random x and y\n",
    "        x, y = stochastic_sample(xs, ys)\n",
    "        \n",
    "        g0 = model.dx_w0(x, y)\n",
    "        g1 = model.dx_w1(x, y)\n",
    "        \n",
    "        G[0][0] = G[0][0] + g0*g0\n",
    "        G[1][1] = G[1][1] + g1*g1\n",
    "        \n",
    "        model.w0 = model.w0 - (learning_rate/np.sqrt(G[0][0] + eps)) * g0\n",
    "        model.w1 = model.w1 - (learning_rate/np.sqrt(G[1][1] + eps)) * g1\n",
    "    \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}\")\n",
    "            print(model)\n",
    "            \n",
    "def RMSprop(model, xs, ys, learning_rate = 0.01, decay_factor = 0.9, max_num_iteration = 10000, eps=0.0000001):\n",
    "    \"\"\"\n",
    "        RMSprop: will estimate the parameters w0 and w1 \n",
    "        (here it uses least square cost function)\n",
    "        model: the model we are trying to optimize using sgd\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        learning_rate: the learning rate for the step that weights update will take\n",
    "        decay_factor: the parameter used in the running averaging\n",
    "        max_num_iteration: the number of iteration before we stop updating\n",
    "        eps: is a numerical safety to avoid division by 0\n",
    "    \"\"\"         \n",
    "    \n",
    "    # Running average\n",
    "    E = [0,0]\n",
    "    \n",
    "    for i in range(max_num_iteration):\n",
    "        \n",
    "        # Select a random x and y\n",
    "        x, y = stochastic_sample(xs, ys)\n",
    "        \n",
    "        g0 = model.dx_w0(x, y)\n",
    "        g1 = model.dx_w1(x, y)\n",
    "        \n",
    "        E[0] = decay_factor*E[0] + (1-decay_factor)*g0*g0\n",
    "        E[1] = decay_factor*E[1] + (1-decay_factor)*g1*g1\n",
    "        \n",
    "        model.w0 = model.w0 - (learning_rate/np.sqrt(E[0] + eps)) * g0\n",
    "        model.w1 = model.w1 - (learning_rate/np.sqrt(E[1] + eps)) * g1\n",
    "    \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}\")\n",
    "            print(model)\n",
    "\n",
    "def adadelta(model, xs, ys, decay_factor = 0.9, max_num_iteration = 10000, eps=0.0000001):\n",
    "    \"\"\"\n",
    "        Adadelta: will estimate the parameters w0 and w1\n",
    "        model: the model we are trying to optimize using sgd\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        decay_factor: the parameter used in the running averaging\n",
    "        max_num_iteration: the number of iteration before we stop updating\n",
    "        eps: is a numerical safety to avoid division by 0\n",
    "    \"\"\"         \n",
    "    \n",
    "    # Running average\n",
    "    E_g = [0,0] # for gradient\n",
    "    E_p = [0,0] # for parameters\n",
    "    delta_p = [0,0] #delta for parameter\n",
    "    \n",
    "    for i in range(max_num_iteration):\n",
    "        \n",
    "        # Select a random x and y\n",
    "        x, y = stochastic_sample(xs, ys)\n",
    "        \n",
    "        g0 = model.dx_w0(x, y)\n",
    "        g1 = model.dx_w1(x, y)\n",
    "        \n",
    "        # Get the running average for the gradient\n",
    "        E_g[0] = decay_factor*E_g[0] + (1-decay_factor)*g0*g0\n",
    "        E_g[1] = decay_factor*E_g[1] + (1-decay_factor)*g1*g1\n",
    "        \n",
    "        # Get the running average for the parameters\n",
    "        E_p[0] = decay_factor*E_p[0] + (1-decay_factor)*delta_p[0]*delta_p[0]\n",
    "        E_p[1] = decay_factor*E_p[1] + (1-decay_factor)*delta_p[1]*delta_p[1]\n",
    "        \n",
    "        # Calculate the gradient difference\n",
    "        delta_p[0] = - np.sqrt(E_p[0] + eps) / np.sqrt(E_g[0] + eps) * g0\n",
    "        delta_p[1] = - np.sqrt(E_p[1] + eps) / np.sqrt(E_g[1] + eps) * g1\n",
    "        \n",
    "        # update the models\n",
    "        model.w0 = model.w0 + delta_p[0]\n",
    "        model.w1 = model.w1 + delta_p[1]\n",
    "        \n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}\")\n",
    "            print(model)\n",
    "            \n",
    "\n",
    "def adam(model, xs, ys, learning_rate = 0.1, b1 = 0.9, b2 = 0.999, epsilon = 0.00000001, max_iteration = 1000):\n",
    "    \"\"\"\n",
    "        Adam: This is the adam optimizer that build upong adadelta and RMSProp\n",
    "        model: The model we want to optimize the parameter on\n",
    "        xs: the feature of my dataset\n",
    "        ys: the continous value (target)\n",
    "        learning_rate: the amount of learning we want to happen at each time step (default is 0.1 and will be updated by the optimizer)\n",
    "        b1: this is the first decaying average with proposed default value of 0.9 (deep learning purposes)\n",
    "        b2: this is the second decaying average with proposed default value of 0.999 (deep learning purposes)\n",
    "        epsilon: a variable for numerical stability during the division\n",
    "        max_iteration: the number of sgd round we want to do before stopping the optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Variable Initialization\n",
    "    m = [0, 0] # two m for each parameter\n",
    "    v = [0, 0] # two v for each parameter\n",
    "    g = [0, 0] # two gradient\n",
    "    t = 1 # time steps\n",
    "    \n",
    "    for i in range(max_iteration):\n",
    "        # Calculate the gradients \n",
    "        x, y = stochastic_sample(xs, ys)\n",
    "        g[0] = model.dx_w0(x, y)\n",
    "        g[1] = model.dx_w1(x, y)\n",
    "\n",
    "        # Update the m and v parameter\n",
    "        m = [b1*m_i + (1 - b1)*g_i for m_i, g_i in zip(m, g)]\n",
    "        v = [b2*v_i + (1 - b2)*(g_i**2) for v_i, g_i in zip(v, g)]\n",
    "\n",
    "        # Bias correction for m and v\n",
    "        m_cor = [m_i / (1 - (b1**t)) for m_i in m]\n",
    "        v_cor = [v_i / (1 - (b2**t)) for v_i in v]\n",
    "\n",
    "        # Update the parameter\n",
    "        model.w0 = model.w0 - (learning_rate / (np.sqrt(v_cor[0]) + epsilon))*m_cor[0]\n",
    "        model.w1 = model.w1 - (learning_rate / (np.sqrt(v_cor[1]) + epsilon))*m_cor[1]\n",
    "\n",
    "        t = t + 1\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}\")\n",
    "            print(model)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam\n",
      "Iteration 0\n",
      "y = 0.8575880576520295 + 0.8405747654640998*x\n",
      "Iteration 100\n",
      "y = -0.001590081073315357 + 0.9984342651063493*x\n",
      "Iteration 200\n",
      "y = 2.70122392056152e-05 + 1.0001344335031852*x\n",
      "Iteration 300\n",
      "y = -1.4876611515003004e-06 + 1.0000033575642253*x\n",
      "Iteration 400\n",
      "y = 1.030624925582267e-05 + 0.9999970769162957*x\n",
      "Iteration 500\n",
      "y = 2.567876097403007e-06 + 0.9999995341571982*x\n",
      "Iteration 600\n",
      "y = 3.001062476619204e-08 + 0.9999999996624379*x\n",
      "Iteration 700\n",
      "y = 3.5509149827671966e-09 + 0.9999999957864142*x\n",
      "Iteration 800\n",
      "y = 2.013987394989773e-08 + 0.9999999952565595*x\n",
      "Iteration 900\n",
      "y = -2.4103486747254733e-08 + 0.9999999984873583*x\n",
      "y = 4.7124723585610135e-07 + 0.9999992952944199*x\n"
     ]
    }
   ],
   "source": [
    "# Here we have a simple line with intercept = 0 and slope = 1\n",
    "xs = [1,2,3,4,5,6,7]\n",
    "ys = [1,2,3,4,5,6,7]\n",
    "\n",
    "\n",
    "# Gradient Descent\n",
    "model = Line()\n",
    "print(\"Gradient Descent: \")\n",
    "gd(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "model = Line()\n",
    "print(\"Stochastic Gradient Descent: \")\n",
    "sgd(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Stochastic Gradient Descent with Momentum\n",
    "model = Line()\n",
    "print(\"SGD + Momentum: \")\n",
    "sgd_momentum(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Adagrad\n",
    "model = Line()\n",
    "print(\"Adagrad\")\n",
    "adagrad(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# RMSprop\n",
    "model = Line()\n",
    "print(\"RMSprop\")\n",
    "RMSprop(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Adadelta\n",
    "model = Line()\n",
    "print(\"Adadelta\")\n",
    "adadelta(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "\n",
    "# Adam\n",
    "model = Line()\n",
    "print(\"Adam\")\n",
    "adam(model, xs, ys)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam\n",
      "Iteration 0\n",
      "y = 0.7770901717569257 + 0.9433753832651581*x\n",
      "Iteration 100\n",
      "y = 0.32725296586754515 + 1.900221970247115*x\n",
      "Iteration 200\n",
      "y = 0.004147083459202631 + 1.99892111204854*x\n",
      "Iteration 300\n",
      "y = -3.4688751132678575e-05 + 2.000013509462116*x\n",
      "Iteration 400\n",
      "y = -9.06655931188767e-08 + 1.9999999764669432*x\n",
      "Iteration 500\n",
      "y = 1.8387422421644338e-10 + 2.0000000000594307*x\n",
      "Iteration 600\n",
      "y = 4.361065425337935e-12 + 1.9999999999993485*x\n",
      "Iteration 700\n",
      "y = 1.1991224399817034e-13 + 1.9999999999999527*x\n",
      "Iteration 800\n",
      "y = -2.5150603804494032e-17 + 2.000000000000001*x\n",
      "Iteration 900\n",
      "y = -8.298690846122729e-17 + 2.0*x\n",
      "y = 9.891579815200104e-17 + 2.0*x\n"
     ]
    }
   ],
   "source": [
    "# Here we have a simple line with intercept = 0 and slope = 2\n",
    "xs = [1,2,3,4,5,6,7]\n",
    "ys = [2,4,6,8,10,12,14]\n",
    "\n",
    "\n",
    "# Gradient Descent\n",
    "model = Line()\n",
    "print(\"Gradient Descent: \")\n",
    "gd(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "model = Line()\n",
    "print(\"Stochastic Gradient Descent: \")\n",
    "sgd(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Stochastic Gradient Descent with Momentum\n",
    "model = Line()\n",
    "print(\"SGD + Momentum: \")\n",
    "sgd_momentum(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Adagrad\n",
    "model = Line()\n",
    "print(\"Adagrad\")\n",
    "adagrad(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# RMSprop\n",
    "model = Line()\n",
    "print(\"RMSprop\")\n",
    "RMSprop(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Adadelta\n",
    "model = Line()\n",
    "print(\"Adadelta\")\n",
    "adadelta(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Adam\n",
    "model = Line()\n",
    "print(\"Adam\")\n",
    "adam(model, xs, ys)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam\n",
      "Iteration 0\n",
      "y = 0.7139491962653317 + 0.6462402260655279*x\n",
      "Iteration 100\n",
      "y = 1.5012704143511661 + 1.913754894630153*x\n",
      "Iteration 200\n",
      "y = 1.147275006702479 + 1.9626892285633653*x\n",
      "Iteration 300\n",
      "y = 1.0187321764566213 + 1.9959358177402393*x\n",
      "Iteration 400\n",
      "y = 1.0018116802276642 + 1.9995771873157036*x\n",
      "Iteration 500\n",
      "y = 1.0001550519477989 + 1.9999713016943046*x\n",
      "Iteration 600\n",
      "y = 1.0000046235977667 + 1.9999989555915936*x\n",
      "Iteration 700\n",
      "y = 0.9999999689046957 + 2.0000000091643306*x\n",
      "Iteration 800\n",
      "y = 1.0000000007114835 + 1.999999999822266*x\n",
      "Iteration 900\n",
      "y = 1.000000000010868 + 2.0000000000003593*x\n",
      "y = 0.9999999999999609 + 2.000000000000028*x\n"
     ]
    }
   ],
   "source": [
    "# Here we have a simple line with intercept = 1 and slope = 2\n",
    "xs = [1,2,3,4,5,6,7]\n",
    "ys = [3,5,7,9,11,13,15]\n",
    "\n",
    "# Gradient Descent\n",
    "model = Line()\n",
    "print(\"Gradient Descent: \")\n",
    "gd(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "model = Line()\n",
    "print(\"Stochastic Gradient Descent: \")\n",
    "sgd(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Stochastic Gradient Descent with Momentum\n",
    "model = Line()\n",
    "print(\"SGD + Momentum: \")\n",
    "sgd_momentum(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Adagrad\n",
    "model = Line()\n",
    "print(\"Adagrad\")\n",
    "adagrad(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# RMSprop\n",
    "model = Line()\n",
    "print(\"RMSprop\")\n",
    "RMSprop(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Adadelta\n",
    "model = Line()\n",
    "print(\"Adadelta\")\n",
    "adadelta(model, xs, ys)\n",
    "print(model)\n",
    "\n",
    "# Adam\n",
    "model = Line()\n",
    "print(\"Adam\")\n",
    "adam(model, xs, ys)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
